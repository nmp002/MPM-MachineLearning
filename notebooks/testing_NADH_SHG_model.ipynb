{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "from models.microscopy_cnn import MicroscopyCNN\n",
    "import torch\n",
    "from scripts.dataset_loader import MicroscopyDataset\n",
    "from torch.utils.data import DataLoader"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = MicroscopyCNN(in_channels=2, task='classification')\n",
    "\n",
    "# model.load_state_dict(torch.load(r'C:\\Users\\nmp002\\PycharmProjects\\MPM-MachineLearning\\trained_models\\best_NADH_only_24FEB2025.pt', map_location=torch.device('cpu'),weights_only=True))\n",
    "model.load_state_dict(torch.load(r'C:\\Users\\nmp002\\PycharmProjects\\MPM-MachineLearning\\trained_models\\epoch250_NADH_SHG.pt', map_location=torch.device('cpu'),weights_only=True))"
   ],
   "id": "a55b39c11dd1f2a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load data\n",
    "csv_file=r\"C:\\Users\\nmp002\\PycharmProjects\\MPM-MachineLearning\\data\\newData\\labels.csv\"\n",
    "root_dir=r\"C:\\Users\\nmp002\\PycharmProjects\\MPM-MachineLearning\\data\\newData\"\n",
    "\n",
    "dataset = MicroscopyDataset(\n",
    "    csv_file=csv_file,\n",
    "    root_dir=root_dir,\n",
    "    # channels = ['nadh'],\n",
    "    channels = ['nadh', 'shg'],\n",
    "    transform=None\n",
    ")"
   ],
   "id": "f8c48aaf07ca03cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "specific_test_paths = []\n",
    "# sample_indices = [23,0,3,20]\n",
    "sample_indices = range(0,29)\n",
    "eval_indices = [27, 4, 25, 7, 8]\n",
    "for sample_index in eval_indices:\n",
    "    if sample_index not in [25,7]:\n",
    "        specific_test_paths.append(dataset.sample_wise_paths[sample_index])\n",
    "\n",
    "print(specific_test_paths)\n",
    "# print('\\n------------------------------------------\\n')\n",
    "# print(dataset.sample_wise_paths)"
   ],
   "id": "1b77e2c009e35a02",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def score_em(t, o):\n",
    "    fpr, tpr, thresholds = roc_curve(t, o)\n",
    "    score = auc(fpr, tpr)\n",
    "    score_all = roc_auc_score(t, o)\n",
    "    print(score_all)\n",
    "    thresh = thresholds[np.argmax(tpr - fpr)]\n",
    "    print(type(thresholds), type(thresh))\n",
    "    preds = [out >= thresh for out in o]\n",
    "    print(f\"Predictions: {preds}\")\n",
    "    for tar,pred in zip(t, preds):\n",
    "        print(f'{tar} -> {pred}')\n",
    "    RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=score).plot()\n",
    "    ConfusionMatrixDisplay.from_predictions(t, preds).plot()"
   ],
   "id": "a5c055496e6129c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import auc, roc_curve, confusion_matrix, RocCurveDisplay, ConfusionMatrixDisplay\n",
    "from scripts.dataset_loader import tiff_to_tensor\n",
    "import numpy as np\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    targets = []\n",
    "    all_targets = []\n",
    "    outs = []\n",
    "    all_outs = []\n",
    "    for sample in specific_test_paths:\n",
    "        fov_outs = []\n",
    "        if sample:\n",
    "            for fov in sample:\n",
    "                # De-nest fov paths and get the indexed item path\n",
    "                score = fov[1]\n",
    "                combined_image = torch.cat([tiff_to_tensor(channel) for channel in fov[0]], dim=0).unsqueeze(0)\n",
    "                y = model(combined_image).item()\n",
    "                all_outs.append(y)\n",
    "                fov_outs.append(y)\n",
    "                all_targets.append(1 if score > 25 else 0)\n",
    "            targets.append(1 if score > 25 else 0)\n",
    "            outs.append(np.min(fov_outs))\n",
    "# score_em(all_targets, all_outs)\n",
    "# score_em(targets, outs)"
   ],
   "id": "d568c1f7ec85984",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(all_targets)\n",
    "print(outs)"
   ],
   "id": "e60becac4a3fec70",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "input_channels = ['nadh','shg']\n",
    "# Load full dataset\n",
    "full_dataset = MicroscopyDataset(\n",
    "    csv_file=\"../data/newData/labels.csv\",\n",
    "    root_dir=\"../data/newData\",\n",
    "    channels = input_channels,\n",
    "    transform=None\n",
    ")"
   ],
   "id": "db237d663fc83ec8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "8e70e2bce0fdb45f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "samples_list = full_dataset.sample_wise_paths\n",
   "id": "750c1d3d99755667",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import random\n",
    "random.shuffle(samples_list)\n",
    "print(samples_list)"
   ],
   "id": "4afc96a9cd5b7abf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torchvision.transforms.v2 as tvt\n",
    "# Transformations for training set\n",
    "train_transform = tvt.Compose([\n",
    "    tvt.RandomVerticalFlip(p=0.25),\n",
    "    tvt.RandomHorizontalFlip(p=0.25),\n",
    "    tvt.RandomRotation(degrees=(-180, 180))])\n",
    "full_dataset.transform = train_transform"
   ],
   "id": "879c16df30debe74",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-05T20:35:09.596428Z",
     "start_time": "2025-03-05T20:35:09.592941Z"
    }
   },
   "cell_type": "code",
   "source": [
    "total_samples = len(samples_list)\n",
    "train_size = int(0.7 * total_samples)\n",
    "val_size = int(0.2 * total_samples)\n",
    "test_size = total_samples - train_size - val_size\n",
    "\n",
    "indices = torch.utils.data.SubsetRandomSampler(range(len(samples_list)))\n",
    "print(indices)\n",
    "indices = [i for i in indices]\n",
    "print(indices)"
   ],
   "id": "1f6c7691535fd89c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.sampler.SubsetRandomSampler object at 0x000001AD86C08E10>\n",
      "[19, 17, 25, 8, 2, 3, 0, 1, 15, 5, 28, 23, 9, 4, 6, 10, 26, 12, 7, 27, 14, 21, 20, 18, 24, 11, 13, 22, 16]\n"
     ]
    }
   ],
   "execution_count": 133
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for i in indices:\n",
    "    if samples_list[i][0][1] > 25:\n",
    "        if i+1 < 21:\n",
    "            print(f\"Sample_{(i+1):03} has a HIGH score\")\n",
    "        else:\n",
    "            print(f\"Sample_{(i+2):03} has a HIGH score\")\n",
    "    else:\n",
    "        if i+1 < 21:\n",
    "            print(f\"Sample_{(i+1):03} has a LOW score\")\n",
    "        else:\n",
    "            print(f\"Sample_{(i+2):03} has a LOW score\")"
   ],
   "id": "8aae929eb0655f00",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(indices)",
   "id": "c664618d7c34db8b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sample_id = full_dataset.data_frame['sample_id'].iloc[9]\n",
    "for i, fov_path in enumerate(full_dataset._denest()):\n",
    "        print(fov_path[0][0])"
   ],
   "id": "57f3ec3464fca58b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(indices)\n",
    "train_samples = indices[:train_size]\n",
    "print(f\"Train samples: {train_samples}\")\n",
    "train_indices = [full_dataset.get_sample_indices(sample) for sample in train_samples]\n",
    "train_indices = [i for sublist in train_indices for i in sublist]\n",
    "print(f\"Train indices: {train_indices}\")\n",
    "train_data = torch.utils.data.Subset(full_dataset, train_indices)\n",
    "\n",
    "\n",
    "val_samples = indices[train_size:train_size+val_size]\n",
    "print(f\"Val samples: {val_samples}\")\n",
    "val_indices = [full_dataset.get_sample_indices(sample) for sample in val_samples]\n",
    "val_indices = [i for sublist in val_indices for i in sublist]\n",
    "print(f\"Val indices: {val_indices}\")\n",
    "val_data = torch.utils.data.Subset(full_dataset, val_indices)\n",
    "\n",
    "test_samples = indices[train_size+val_size:]\n",
    "print(f\"Test samples: {test_samples}\")\n",
    "test_indices = [full_dataset.get_sample_indices(sample) for sample in test_samples]\n",
    "test_indices = [i for sublist in test_indices for i in sublist]\n",
    "print(f\"Test indices: {test_indices}\")\n",
    "test_data = torch.utils.data.Subset(full_dataset, test_indices)"
   ],
   "id": "84d2d0aa054bff7a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from scripts.dataset_loader import tiff_to_tensor\n",
    "for i in test_samples:\n",
    "    for sample_path in full_dataset.sample_wise_paths[i]:\n",
    "        if sample_path:\n",
    "            for fov in sample_path:\n",
    "                score = fov[1]\n",
    "                combined_image = torch.cat([tiff_to_tensor(channel) for channel in fov[0]], dim=0).unsqueeze(0)"
   ],
   "id": "24fad372b2ddd332",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dataloaders = {\n",
    "    'train': DataLoader(train_data, batch_size=16, shuffle=True),\n",
    "    'val': DataLoader(val_data, batch_size=16, shuffle=False),\n",
    "    'test': DataLoader(test_data, batch_size=len(test_data), shuffle=False)\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "\n",
    "    for img, targets in dataloaders['test']:\n",
    "        print(targets)\n",
    "        print(img.shape)\n",
    "        y = model(img).squeeze()\n",
    "\n",
    "targets = [1 if t > 25 else 0 for t in targets]\n",
    "print(targets)\n",
    "print(y)\n",
    "y = y.numpy().astype(np.float64).tolist()\n",
    "print(y)\n",
    "# score_em(targets, y)"
   ],
   "id": "8bcf83d54636bf76",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "1fda426d9a8d54bd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fpr, tpr, thresholds = roc_curve(targets, y)\n",
    "print(f\"fpr: {fpr}, tpr: {tpr}, thresholds: {thresholds}\")"
   ],
   "id": "95f6d6f61fad0a6c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_score = auc(fpr, tpr)\n",
    "print(test_score)"
   ],
   "id": "783dcfd1b3ff4dfc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "thresh = thresholds[np.argmax(tpr - fpr)]\n",
    "print(f\"Thresh: {thresh}\")"
   ],
   "id": "28122d556f20230b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "preds = [out >= thresh for out in y]\n",
    "print(f\"Predictions: {preds}\")"
   ],
   "id": "d32b1b590e18bfed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=test_score).plot()",
   "id": "2f2ed74aa1f66bd6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "conf_matrix = ConfusionMatrixDisplay.from_predictions(targets, preds).confusion_matrix",
   "id": "32e9192131e67eaf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "total_samples = len(samples_list)\n",
    "train_size = int(0.7 * total_samples)\n",
    "val_size = int(0.2 * total_samples)\n",
    "test_size = total_samples - train_size - val_size\n",
    "print(f\"Total samples: {total_samples}, train_size: {train_size}, val_size: {val_size}, test_size: {test_size}\")"
   ],
   "id": "64a051a4d22e2a7d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_samples = samples_list[:train_size]\n",
    "val_samples = samples_list[train_size:train_size+val_size]\n",
    "test_samples = samples_list[train_size+val_size:]\n",
    "print(f\"Train samples: {train_samples}\\n\\nval samples: {val_samples}\\n\\ntest samples: {test_samples}\")"
   ],
   "id": "8e52bf14ab23917e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for sample in test_data:\n",
    "    for fov in sample:\n",
    "        score = fov[1]\n",
    "        print(score)"
   ],
   "id": "43727d3d3fb0dd40",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "3f651122479ece4f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
